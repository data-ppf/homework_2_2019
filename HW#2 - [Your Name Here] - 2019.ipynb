{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 \n",
    "# Correlation, Regression, & Significance!\n",
    "<b>ASSIGNED: Friday, Mar 1 2019.</b>   \n",
    "<b>DUE: Friday, Mar 15 2019 at 6 pm</b> (Please be aware that we extended the deadline so that you can attend an additional office hours session with Will on Friday, Mar 15, 2019.) \n",
    "\n",
    "NOTE: If you're on the STEM student track, do not complete either homework #1 or homework #2 (i.e., this homework) for a grade. You may still do this homework if you are unfamiliar python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose and Context:\n",
    "We're going to spend some time thinking through claims of causation, correlation, and clustering. We've seen arguments about causation and regression in lab 4. We used PCA in lab 5 to examine high-dimensional data in a two-dimensional plot. Finally, in lab 6, you learned about Fisher's arguments for significance testing, including t-tests, and tried your hand at p-hacking.  In this homework, you're going to get more familar with some of the statistical tools, reflect upon the assumptions of these tools, and, as always, reflect upon the provenance of the data we're using. As we've seen in lab 6, arguments about brain size and intelligence wasn't just happening in the late 1800s. (Recall the long history of using brain size to argue for different racial differences in intelligence, as discussed by Gould's *The Mismeasure of Man* and others.) It's happening today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "This assignment is to be done <b>on your own</b>, but you can talk about the assignment with your classmates if you get stuck. (Be sure to list the students you spoke with about this assignment in the space provided below.) Feel free to also use [stackoverflow](https://stackoverflow.com/) but please provide citation and link to the specific answer if you do this. You may also visit Will Yumou during his TA office hours or email hime with questions. If you get stuck on a problem for more than 30 minutes, be sure to ask your classmates for help! \n",
    "\n",
    "Provide code as required to justify your answer to each question.\n",
    "\n",
    "Be sure to rename this homework notebook so that it includes your name.\n",
    "\n",
    "### List any students you talked with about this assignment here:\n",
    "1. [person 1]\n",
    "2. [person 2]\n",
    "3. etc.\n",
    "\n",
    "\n",
    "# Homework Problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 1 (10 points) + extra credit (5 points) \n",
    "\n",
    "Let's begin by downloading the data we used in lab 6. Some code has been provided to get you started. \n",
    "\n",
    "Here's where the data is located:\n",
    "`http://www.scipy-lectures.org/_downloads/brain_size.csv`\n",
    "\n",
    "Do the following:\n",
    "1. Just like we did in lab 6, download this data using `read_csv`. \n",
    "2. Use `head()` to get famailar with the data. \n",
    "3. Then use `describe()` to get a sense of how the data is distributed. (Note that \"MRI_count\" here is a measure of brain size.)\n",
    "4. Determine if there are any rows that contain NANs. If so, drop missing rows using `dropna()` and name the resultant NAN-free data frame `people`. \n",
    "\n",
    "\n",
    "<b>Extra Credit (5 points)</b>: Track down some useful (i.e., citable) information about where this data set came from, where it was used, or how it was produced. Explain this. (Be sure to write your extra credit answer in a Markdown notebook cell.)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # you'll need this!\n",
    "\n",
    "# write your answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the usual libraries, you'll also need the stats library for this homework. Let's just add it now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### question 1 extra credit\n",
    "\n",
    "[write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 2 (15 points) + extra credit (5 points)\n",
    "To get a better sense of the data we're using, let's perform PCA on this data set. The useful thing PCA does, besides reducing your data to two-dimensions which are easy to visualize, is that it helps you plot your data in such a way as to maximize its variance, i.e., it helps give a sense of our data is clustered. We used PCA in lab 5b to cluster different kinds of Iris flowers and to examine Spearman's student IQ data. \n",
    "\n",
    "<b>Now let's perform PCA on this data set and graph it.</b> \n",
    "1. Perform PCA on the data. (Hint: You'll need three lines of code to do this. See the following two cells of code in lab 4 with the first lines \"## make table of iris data X\" and \"# perform pca, pick number of dimensions...\" for an example of how to do this. You will need to modify the code as appropriate.) Name the resultant coordinates `pca_coordinates`. We've given you some code to start. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Now run the code below. It will add the x and y pca coordinates as new columns in your dataframe `people`, color code the data points by gender (i.e., m = red, f = blue), and then plot the pca graph. <b>Pandas makes plotting much easier. Use it instead of matplotlib when possible.</b> (Note the code will give warnings, but you can ignore them. You will need to run the cell twice to get it to display properly!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code after you finish question 2.1 above \n",
    "# Run this cell TWICE to get the graph to display properly!!\n",
    "\n",
    "x_pca_coordinate, y_pca_coordinate = zip(*pca_coordinates)\n",
    "people['x pca coordinate']=x_pca_coordinate\n",
    "people['y pca coordinate']=y_pca_coordinate\n",
    "\n",
    "colors= {'Female': 'blue', 'Male': 'red'}\n",
    "people.plot.scatter('x pca coordinate', 'y pca coordinate', c=people['Gender'].apply(lambda x: colors[x]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extra Credit (5 points)</b>: What, if anything, does this PCA plot suggest about the representativeness of this data set? (Note that gender is represented by color in this plot.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Please provide your extra credit answer here.]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 3 (10 points) + extra credit (3 points)\n",
    "\n",
    "Pearson correlation coefficient $r$ measures the linear relationship between two data sets, and <b>assumes</b> that both datasets are gaussian-distributed. It will produce a value between +1 to -1, where +1 is a completely-correlated one-to-one relationship, 0 is uncorrelated, and -1 is an anticorrelated one-to-one relationship (i.e., x increases as y decreases). The `st.pearsonr` function returns both r and its corresponding p-value.\n",
    "\n",
    "\n",
    "1. <b>Compute r for FSIQ vs VIQ, Weight vs Height, and MRI_count vs PIQ using `st.pearsonr`. \n",
    "    \n",
    "<b> Extra Credit (3 points)</b>: What does the correlation coefficient $r$ assume that is not true for our data? \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extra credit answer\n",
    "\n",
    "[write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 4 (10 points) + extra credit (2 points)\n",
    "From question 3, we note a middling correlation between Weight and Height and a strong correlation between FSIQ and VIQ. \n",
    "\n",
    "1. Perform linear regression for both Weight vs Height and FSIQ vs VIQ. (Recall we did linear regression in lab 4 using the `sklearn` library. Please see our code there for example code.) \n",
    "\n",
    "<b>Extra Credit (2 points)</b>: What do you notice about your regression of FSIQ vs VIQ that should make you suspicious of the result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4 answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extra credit answer \n",
    "[write your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional observations about causality and linear regression you should know:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to argue that the linear regression you performed demonstrates a causal relationship, you need to make several assumptions. \n",
    "\n",
    "First, for our regressions to indicate causality, we need to assume that our variable y fully explains the observations of X. However, it's certaintly the case that other variables also influence y, and any causal claims effectively need to take into account all the causal variables involved (i.e., we need to be sure there are no \"confounding variables\"). Determining what all the causal variables are is a very difficult question to answer for most interesting questions! \n",
    "\n",
    "Related to this, we need to be sure that our y does not have a causal influence on X.  \n",
    "\n",
    "We also need to beware of statistical bias we might introduce by our selection and/or construction of data: we need to randomly draw from a \"representative\" sample population if we wish to make a causal claim for that population, but if you did question 1 extra credit you know we specifically didn't do that. \n",
    "\n",
    "Finally, if we're going to make causal claims, we also need to have some kind of control or clearly defined null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 5 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Student's t distribution is actually a family of distributions, which converges to a gaussian distribution as the number of samples $n \\rightarrow \\infty$. What we actually want is to determine the gaussian-distributed population but in practice we don't know what that is because all we have is a sample drawn from that distribution. This sample will give us a t distribution. The t distribution can be used to test a hypothesis that a sample comes from a gaussian-distributed population. What are the assumptions? \n",
    "1. One or both samples come from normally-distributed populations\n",
    "2. The variance is the same for both populations\n",
    "As already noted, the t distribution for a sample changes with sample size (i.e., the number of data points).\n",
    "\n",
    "For a 1-sample t-test, you are trying to decide if your sample came from a population with a particular mean. For this test, you specify the mean as the null hypothesis. Using the resultant p-value and a chosen threshold value (e.g., 1%, 5%, etc.) we can reject the null hypothesis. You can perform this test using `st.ttest_1samp()` in python.     \n",
    "\n",
    "For an independent t-test, you're trying to decide how likely the two samples you have come from a population with the same mean (assuming that the variances of these populations are equal). (ASIDE: if you don't want to assume equal population variances, you want a Welch's t-test which you can with this function by setting equal_var = False.) Your null hypothesis is that both populations have equal averages. Using the resultant p-value and a chosen threshold value can allow use to reject the null hypothesis. You can perform this test using `st.ttest_ind()` in python.       \n",
    "\n",
    "(There's also a paired-sample t-test where you compare the same group at two different times, but we won't worry about that here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the following test: <code>st.ttest_1samp(data['VIQ'], 100)</code>. What is this asking and how should the result be interpreted?\n",
    "\n",
    "2. Run the following test: <code>st.ttest_1samp(data['FSIQ'] - data['PIQ'], 0)</code>. What is this asking and how should the result be interpreted? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5 answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question 6 (15 points)\n",
    "What are p-values and what is p-hacking? (See, for instance,  http://www.nature.com/news/scientific-method-statistical-errors-1.14700 ). What are type 1 and type 2 errors? What did Fisher, Neymann, and Egon Pearson think about p-values, type 1 and type 2 error? Who came up with what? (Keep your answer under 200 words.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Question 6 answer\n",
    "[write your answer here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
